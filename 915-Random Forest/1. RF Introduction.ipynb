{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "* Measuring Impurity\n",
    "   * Entropy\n",
    "   * Gini Index\n",
    "   * Classifictaion Error\n",
    "* Bias Variance Tradeoff\n",
    "   * Relationship with RF and Boosting\n",
    "* Random Forests\n",
    "   * Bagging\n",
    "   * Disadvantages of Bagging\n",
    "   * Random Forest algorithm\n",
    "   * Gini importance\n",
    "   * Missing Value Train/Test \n",
    "   * Advantages and Disadvantages of Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Measuring Impurity\n",
    "\n",
    "There are several indices to quantify degree of impurity. Most used measures are Entropy, Gini Index and Classification error. They can be understood by below formula:\n",
    "![](d1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pj is probability of class J\n",
    "![](d2.png)\n",
    "![](d3.png)\n",
    "![](en.png)\n",
    "![](gi.png)\n",
    "![](cl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias Variance Tradeoff\n",
    "\n",
    "The biasâ€“variance tradeoff is the problem of simultaneously minimizing two sources of error that prevent supervised learning algorithms from generalizing beyond their training set.\n",
    "* High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).\n",
    "* High variance can cause overfitting: modeling the random noise in the training data, rather than the intended outputs.\n",
    "\n",
    "![](bv.png)\n",
    "![](bv2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error due to Bias:** The error due to bias is taken as the difference between the expected (or average) prediction of our model and the correct value which we are trying to predict. Of course you only have one model so talking about expected or average prediction values might seem a little strange. However, imagine you could repeat the whole model building process more than once: each time you gather new data and run a new analysis creating a new model. Due to randomness in the underlying data sets, the resulting models will have a range of predictions. Bias measures how far off in general these models' predictions are from the correct value.\n",
    "\n",
    "**Error due to Variance:** The error due to variance is taken as the variability of a model prediction for a given data point. Again, imagine you can repeat the entire model building process multiple times. The variance is how much the predictions for a given point vary between different realizations of the model.\n",
    "![](bv3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relationship with RF and Boosting \n",
    "\n",
    "**error=bias+variance **\n",
    "\n",
    "**Boosting** is based on weak learners (high bias, low variance). In terms of decision trees, weak learners are shallow trees, sometimes even as small as decision stumps (trees with two leaves). Boosting reduces error mainly by reducing bias (and also to some extent variance, by aggregating the output from many models).\n",
    "\n",
    "On the other hand, **Random Forest** uses as you said fully grown decision trees (low bias, high variance). It tackles the error reduction task in the opposite way: by reducing variance. The trees are made uncorrelated to maximize the decrease in variance, but the algorithm cannot reduce bias (which is slightly higher than the bias of an individual tree in the forest). Hence the need for large, unprunned trees, so that the bias is initially as low as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "Random forest is a tree-based algorithm which involves building several trees (decision trees), then combining their output to improve generalization ability of the model. The method of combining trees is known as an ensemble method. Ensembling is nothing but a combination of weak learners (individual trees) to produce a strong learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "\n",
    "* It creates randomized samples of the data set and grows trees on a different sample of the original data. The remaining 1/3 of the sample is used to estimate unbiased OOB error.\n",
    "* It considers all the features at a node (for splitting).\n",
    "* Once the trees are fully grown, it uses averaging or voting to combine the resultant predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disadvantages of bagging\n",
    "\n",
    "The bagging algorithm results in correlated trees when faced with a data set having strong predictors. Unfortunately, averaging several highly correlated trees doesn't lead to a large reduction in variance.\n",
    "\n",
    "Let's say a data set has a very strong predictor, along with other moderately strong predictors. In bagging, a tree grown every time would consider the very strong predictor at its root node, thereby resulting in trees similar to each other.\n",
    "\n",
    "The main difference between random forest and bagging is that random forest considers **only a subset of predictors** at a split. This results in trees with different predictors at top split, thereby resulting in decorrelated trees and more reliable average output. That's why we say random forest is robust to correlated predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Algorithm\n",
    "\n",
    "Building many decision trees results in a forest. A random forest works the following way:\n",
    "\n",
    "* First, it uses the Bagging (Bootstrap Aggregating) algorithm to create random samples. Given a data set D1 (n rows and p columns), it creates a new dataset (D2) by sampling n cases at random with replacement from the original data. About 1/3 of the rows from D1 are left out, known as Out of Bag(OOB) samples.\n",
    "* Then, the model trains on D2. OOB sample is used to determine unbiased estimate of the error.\n",
    "* Out of p columns, P << p columns are selected at each node in the data set. The P columns are selected at random. Usually, the default choice of P is p/3 for regression tree and P is sqrt(p) for classification tree.\n",
    "* Unlike a tree, no pruning takes place in random forest; i.e, each tree is grown fully. In decision trees, pruning is a method to avoid overfitting. Pruning means selecting a subtree that leads to the lowest test errror rate. We can use cross validation to determine the test error rate of a subtree.\n",
    "* Several trees are grown and the final prediction is obtained by averaging or voting.\n",
    "\n",
    "Each tree is grown on a different sample of original data. Since random forest has the feature to calculate OOB error internally, cross validation doesn't make much sense in random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages and Disadvantages of Random Forest\n",
    "\n",
    "Advantages are as follows:\n",
    "\n",
    "* It is robust to correlated predictors.\n",
    "* It is used to solve both regression and classification problems.\n",
    "* It can handle thousands of input variables without variable selection.\n",
    "* It can be used as a feature selection tool using its variable importance plot.\n",
    "* It takes care of missing data internally in an effective manner.\n",
    "\n",
    "Disadvantages are as follows:\n",
    "\n",
    "* The Random Forest model is difficult to interpret.\n",
    "* It tends to return erratic predictions for observations out of range of training data. For example, the training data contains two variable x and  y. The range of x variable is 30 to 70. If the test data has x = 200, random forest would give an unreliable prediction.\n",
    "* It can take longer than expected time to computer a large number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
