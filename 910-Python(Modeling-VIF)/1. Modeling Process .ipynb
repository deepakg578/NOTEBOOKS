{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Process\n",
    "\n",
    "The process is as follows:\n",
    "\n",
    "* Problem Definition: Understand and clearly describe the problem that is being solved.\n",
    "* Analyze Data: Understand the information available that will be used to develop a model.\n",
    "* Prepare Data: Discover and expose the structure in the dataset.\n",
    "* Evaluate Algorithms: Develop a robust test harness and baseline accuracy from which to improve and spot check algorithms.\n",
    "* Improve Results: Leverage results to develop more accurate models.\n",
    "* Present Results: Describe the problem and solution so that it can be understood by third parties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify Data Needs:\n",
    "* Start with Business\n",
    "Question\n",
    "* Determine data need\n",
    "for delivering desired\n",
    "outcome\n",
    "\n",
    "#### Data Mapping\n",
    "* Before preparing a data\n",
    "request, it is necessary\n",
    "to become as familiar\n",
    "as possible with the\n",
    "data sources and their\n",
    "content that might be\n",
    "available to address the\n",
    "business question to be\n",
    "answered.\n",
    "\n",
    "* This Data Mapping has\n",
    "basically three major components:\n",
    "    * Interview clients\n",
    "    * Obtain & study data\n",
    "layouts\n",
    "    * Obtain & evaluate\n",
    "data samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Analyze Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A comprehensive data dictionary should be maintained and updated as and when any new information is gathered.\n",
    "\n",
    "THINGS TO INCLUDE IN THE DATA DICTIONARY:\n",
    "* Meaning of all Potential Predictors:\n",
    "    * Maintain labels of as many variables as possible\n",
    "    * If possible, one should also try to capture the business sense of these variables\n",
    "* Dependent Variable Definition and Meaning\n",
    "* Variable Classification: If not already given, one should always try and classify the variables like\n",
    "    * Demographic variables, e.g. age, gender\n",
    "    * Performance variables, e.g. spend, number of transactions\n",
    "    * Credit Attributes, e.g. total credit line, FICO score\n",
    "    * Census level, e.g. population, location attributes such as income levels\n",
    "* EDA : Exploratory data analysis:\n",
    "    * Univariate : Single variable -> Mean, STD, histograms, Box Plots\n",
    "    * Bivariate: Two variables -> Correlation, Scatter Plot\n",
    "    * Multivariate: More than two variables -> VIF,scatter Plot, Box Plots with category\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I. Test and Train Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why?\n",
    "* To start the modeling process, there is a need to create modeling and validation datasets.\n",
    "* Validation dataset helps validate the performance of the model which is built using the\n",
    "modeling dataset. \n",
    "* A poor performance on validation dataset would imply that the model\n",
    "is not robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps:\n",
    "* ***Step 1*** : Before we start the modeling process, we need to define and create the modeling population. From the data that is shared by the client, depending upon the scope of the analysis, an assessment of the required data (a certain amount of history, a certain length of future for prediction, quality of data, etc.),list down the defining criterion for eligible population.\n",
    "\n",
    "* ***Step 2***: Split the final eligible population into parts –modeling dataset (also called training dataset) and validation datasets. This can be done using:\n",
    "    * a random assessment (60:40 split or 80:20 split); or\n",
    "    * specific splitting criterion (based on time/segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#require(dplyr)\n",
    "#train<-sample_frac(cars, 0.7)\n",
    "#sid<-as.numeric(rownames(train)) # because rownames() returns character\n",
    "#test<-mtcars[-sid,]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. Identifying Non Usable Variables\n",
    "\n",
    "Even at this early stage one can identify certain variables which can be deemed as ‘non-usable for\n",
    "modeling purpose’. This way we can reduce the dimension of the dataset. Some logics that can be\n",
    "applied are as follows:\n",
    "\n",
    "* Variables with a single unique value throughout the dataset: By definition, such variables have zero explanatory\n",
    "power and hence are irrelevant for any analysis. These variables are usually flags like merge indicators.\n",
    "* ID Variables: Such variables may be needed in the dataset for observation tagging. However, they should NOT be used as\n",
    "predictors in the model.\n",
    "* Variables with very low fill rates:\n",
    "    * Case I: Variable, in question, is defined over a specific segment only. This segment may be used to subset the modeling\n",
    "dataset for developing segment-specific models. In such a case, the same variable is usable for one segment; while\n",
    "non-usable for the other.\n",
    "    * Case II: Missing value may signify something; and may be associated with a meaningful value.\n",
    "    * Case III: Variable fill rate is less than even 50% but there is a strong business case for its inclusion. In this case, the\n",
    "appropriate technique of missing value imputation should be applied.\n",
    "    * Case IV: If none of the above cases holds, some minimum fill rate cut-off may be put for dataset dimension reduction.\n",
    "According to standard modeling conventions, any variable having lower than 50% fill rate is not included in the model.\n",
    "This cut-off for fill rate can be set higher or lower depending on how well populated is the data received.\n",
    "\n",
    "* Variables which cannot be used because of implementation issues should be dropped.\n",
    "* Certain variable like Gender, Ethnicity which cannot be used due to regulatory issues (depending upon the business\n",
    "problem in context) should also be dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III. Reformatting Variables\n",
    "\n",
    "Categorical and continuous variables are treated differently in most of the analysis. Hence, it’s always\n",
    "advisable to separate out possible categorical variables from the continuous ones.\n",
    "Few points to remember:\n",
    "* Look at data description to check variable format.\n",
    "* Check number of unique values. Numerical variables taking only 10-15 unique\n",
    "value may be treated as categorical. It’s a subjective call, depending on the\n",
    "variable and its expected use in model.\n",
    "* Apply business sense before treating variables as continuous / categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV. Outlier Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V. Missing value Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Evaluate Algorithms:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Metrics\n",
    "\n",
    "* Classification Accuracy.\n",
    "    * Classification accuracy is the number of correct predictions made as a ratio of all predictions made.\n",
    "\n",
    "* Logarithmic Loss.\n",
    "    * Logarithmic loss (or logloss) is a performance metric for evaluating the predictions of probabilities of membership to a given class.\n",
    "\n",
    "    * The scalar probability between 0 and 1 can be seen as a measure of confidence for a prediction by an algorithm. Predictions that are correct or incorrect are rewarded or punished proportionally to the confidence of the prediction.\n",
    "\n",
    "* Area Under ROC Curve.\n",
    "    * Area under ROC Curve (or AUC for short) is a performance metric for binary classification problems.\n",
    "\n",
    "    * The AUC represents a model’s ability to discriminate between positive and negative classes. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model as good as random\n",
    "![](ROC.png)\n",
    "* Confusion Matrix.\n",
    "![](mat.png)\n",
    "* Classification Report.\n",
    "    * The classification_report() function displays the precision, recall, f1-score and support for each class.\n",
    "    * Positive Predictive Value or Precision : the proportion of positive cases that were correctly identified.\n",
    "    * Sensitivity or Recall : the proportion of actual positive cases which are correctly identified.\n",
    "    * Specificity : the proportion of actual negative cases which are correctly identified.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression metrics\n",
    "\n",
    "\n",
    "* **R Square (Coefficient of Determination)** - As explained above, this metric explains the percentage of variance explained by covariates in the model. It ranges between 0 and 1. Usually, higher values are desirable but it rests on the data quality and domain. For example, if the data is noisy, you'd be happy to accept a model at low R² values. But it's a good practice to consider adjusted R² than R² to determine model fit.\n",
    "* **Adjusted R²**- The problem with R² is that it keeps on increasing as you increase the number of variables, regardless of the fact that the new variable is actually adding new information to the model. To overcome that, we use adjusted R² which doesn't increase (stays same or decrease) unless the newly added variable is truly useful.\n",
    "\n",
    "* **RMSE / MSE / MAE** - Error metric is the crucial evaluation number we must check. Since all these are errors, lower the number, better the model. Let's look at them one by one:\n",
    " * MSE - This is mean squared error. It tends to amplify the impact of outliers on the model's accuracy. For example, suppose the actual y is 10 and predictive y is 30, the resultant MSE would be (30-10)² = 400.\n",
    " * MAE - This is mean absolute error. It is robust against the effect of outliers. Using the previous example, the resultant MAE would be (30-10) = 20\n",
    " * RMSE - This is root mean square error. It is interpreted as how far on an average, the residuals are from zero. It nullifies squared effect of MSE by square root and provides the result in original units as data. Here, the resultant RMSE would be √(30-10)² = 20. Don't get baffled when you see the same value of MAE and RMSE. Usually, we calculate these numbers after summing overall values (actual - predicted) from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
