{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "* What is Logistic Regression ?\n",
    "* How does Logistic Regression work ?\n",
    "* How can you evaluate Logistic Regression's model fit and accuracy ?\n",
    "* Practical - Who survived on the Titanic ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Logistic Regression ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression belongs to the family of generalized linear models. It is a binary classification algorithm used when the response variable is dichotomous (1 or 0). Inherently, it returns the set of probabilities of target class. But, we can also obtain response labels using a probability threshold value. \n",
    "\n",
    "Following are the assumptions made by Logistic Regression:\n",
    "\n",
    "* The response variable must follow a binomial distribution.\n",
    "* Logistic Regression assumes a linear relationship between the independent variables and the link function (logit).\n",
    "\n",
    "Binomial distribution can be identified by the following characteristics:\n",
    "\n",
    "* There must be a fixed number of trials denoted by n, i.e. in the data set, there must be a fixed number of rows.\n",
    "* Each trial can have only two outcomes; i.e., the response variable can have only two unique categories.\n",
    "* The outcome of each trial must be independent of each other; i.e., the unique levels of the response variable must be independent of each other.\n",
    "* The probability of success (p) and failure (q) should be the same for each trial.\n",
    "\n",
    "In Python, we use sklearn.linear_model function to import and use Logistic Regression.\n",
    "\n",
    "We don't use Linear Regression for binary classification because its linear function results in probabilities outside [0,1] interval, thereby making them invalid predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does Logistic Regression work ?\n",
    "\n",
    "For Linear Regression, where the output is a linear combination of input feature(s), we write the equation as:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                            `Y = βo + β1X + ∈`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Logistic Regression, we use the same equation but with some modifications made to Y. Let's reiterate a fact about Logistic Regression: we calculate probabilities. And, probabilities always lie between 0 and 1. In other words, we can say:\n",
    "\n",
    "* The response value must be positive.\n",
    "* It should be lower than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll meet the above two criteria. We know the exponential of any value is always a positive number. And, any number divided by number + 1 will always be lower than 1. Let's implement these two findings:\n",
    " \n",
    " ![](logistic.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the logistic function.\n",
    "\n",
    "Now we are convinced that the probability value will always lie between 0 and 1. To determine the link function, follow the algebraic calculations carefully. P(Y=1|X) can be read as \"probability that Y =1 given some value for x.\" Y can take only two values, 1 or 0. For ease of calculation, let's rewrite P(Y=1|X) as p(X). \n",
    " \n",
    " ![](equateimage-e1483685096494.png)\n",
    " \n",
    "The right side of the (immediate) equation above depicts the linear combination of independent variables. The left side is known as the log - odds or odds ratio or logit function and is the link function for Logistic Regression. This link function follows a sigmoid (shown below) function which limits its range of probabilities between 0 and 1.\n",
    " ![](SigmoidPlot1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interpret the above equation as, a unit increase in variable x results in multiplying the odds ratio by ε to power β. In other words, the regression coefficients explain the change in log(odds) in the response for a unit change in predictor. However, since the relationship between p(X) and X is not straight line, a unit change in input feature doesn't really affect the model output directly but it affects the odds ratio.\n",
    "\n",
    "This is contradictory to Linear Regression where, regardless of the value of input feature, the regression coefficient always represents a fixed increase/decrease in the model output per unit increase in the input feature.\n",
    "\n",
    "In Multiple Regression, we use the Ordinary Least Square (OLS) method to determine the best coefficients to attain good model fit. In Logistic Regression, we use **maximum likelihood** method to determine the best coefficients and eventually a good model fit.\n",
    "\n",
    "Maximum likelihood works like this: It tries to find the value of coefficients (βo,β1) such that the predicted probabilities are as close to the observed probabilities as possible. In other words, for a binary classification (1/0), maximum likelihood will try to find values of βo and β1 such that the resultant probabilities are closest to either 1 or 0. The likelihood function is written as\n",
    " ![](myloglike-e1483528776450.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can you evaluate Logistic Regression model fit and accuracy ?\n",
    "\n",
    "In Linear Regression, we check adjusted R², F Statistics, MAE, and RMSE to evaluate model fit and accuracy. But, Logistic Regression employs all different sets of metrics. Here, we deal with probabilities and categorical values. Following are the evaluation metrics used for Logistic Regression:\n",
    "\n",
    "**Akaike Information Criteria (AIC)**\n",
    "\n",
    "You can look at AIC as counterpart of adjusted r square in multiple regression. It's an important indicator of model fit. It follows the rule: Smaller the better. AIC penalizes increasing number of coefficients in the model. In other words, adding more variables to the model wouldn't let AIC increase. It helps to avoid overfitting.\n",
    "\n",
    "Looking at the AIC metric of one model wouldn't really help. It is more useful in comparing models (model selection). So, build 2 or 3 Logistic Regression models and compare their AIC. The model with the lowest AIC will be relatively better.\n",
    "![](aic.png)\n",
    "\n",
    "![](aic2.png)\n",
    "\n",
    "**Confusion Matrix**\n",
    "\n",
    "Confusion matrix is the most crucial metric commonly used to evaluate classification models.The skeleton of a confusion matrix looks like this:\n",
    "\n",
    " ![](myprobbb.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In table above, Positive class = 1 and Negative class = 0. Following are the metrics we can derive from a confusion matrix:\n",
    "\n",
    "**Accuracy** - It determines the overall predicted accuracy of the model. It is calculated as Accuracy  = (True Positives + True Negatives)/(True Positives + True Negatives + False Positives + False Negatives)\n",
    "\n",
    "**True Positive Rate (TPR)** - It indicates how many positive values, out of all the positive values, have been correctly predicted. The formula to calculate the true positive rate is (TP/TP + FN). Also, TPR =  1 - False Negative Rate. It is also known as Sensitivity or Recall.\n",
    "\n",
    "**False Positive Rate (FPR)** - It indicates how many negative values, out of all the negative values, have been incorrectly predicted. The formula to calculate the false positive rate is (FP/FP + TN). Also, FPR = 1 - True Negative Rate.\n",
    "\n",
    "**True Negative Rate (TNR) **- It indicates how many negative values, out of all the negative values, have been correctly predicted. The formula to calculate the true negative rate is (TN/TN + FP). It is also known as Specificity.\n",
    "\n",
    "**False Negative Rate (FNR)** - It indicates how many positive values, out of all the positive values, have been incorrectly predicted. The formula to calculate false negative rate is (FN/FN + TP).\n",
    "\n",
    "**Precision**: It indicates how many values, out of all the predicted positive values, are actually positive. It is formulated as:(TP / TP + FP). \n",
    "\n",
    "**Recall**: It indicates how many values, out of all the actual positive values, are actually predicted. It is formulated as:(TP / TP + FN). \n",
    "\n",
    "**F Score**: F score is the harmonic mean of precision and recall. It lies between 0 and 1. Higher the value, better the model. \n",
    "\n",
    "It is formulated as 2((precision*recall) / (precision+recall)).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Receiver Operator Characteristic (ROC)**\n",
    "![](roc4.png)\n",
    "\n",
    "ROC determines the accuracy of a classification model at a user defined threshold value. It determines the model's accuracy using Area Under Curve (AUC). The area under the curve (AUC), also referred to as index of accuracy (A) or concordant index, represents the performance of the ROC curve. Higher the area, better the model. ROC is plotted between True Positive Rate (Y axis) and False Positive Rate (X Axis). \n",
    "\n",
    " ![](roc1.png)\n",
    "\n",
    "In this plot, our aim is to push the red curve (shown below) toward 1 (left corner) and maximize the area under curve. Higher the curve, better the model. The yellow line represents the ROC curve at 0.5 threshold. At this point, sensitivity = specificity.\n",
    " ![](roc.gif)\n",
    " \n",
    " ![](roc2.png)\n",
    " \n",
    " ![](roc3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
