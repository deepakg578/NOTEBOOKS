{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "* Regression\n",
    "* The assumptions made in Regression\n",
    "* How to check if these assumptions are violated\n",
    "* How to improve the accuracy of a Regression Model\n",
    "* How to access the fit of a Regression Model\n",
    "* Solving a Regression Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A regression line is a mathematical formula that quantifies the general relation between a predictor/independent (or known variable x) and the target/dependent (or the unknown variable y).\n",
    "\n",
    "Regression is a parametric technique used to predict continuous (dependent) variable given a set of independent variables. It is parametric in nature because it makes certain assumptions based on the data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the regression line. If we have the data of x and y then we can build a model to generalize their relation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](reg.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y - This is the variable we predict\n",
    "x - This is the variable we use to make a prediction\n",
    "bo - This is the intercept term. It is the prediction value you get when X = 0\n",
    "b1 - This is the slope term. It explains the change in Y when X changes by 1 unit.\n",
    "e - This represents the residual value, i.e. the difference between actual and predicted values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best line will have the minimum error. \n",
    "Some errors are positive and some are negative. Taking their sum is not a good idea.\n",
    "Squared sum of errors are convenient to minimize. The method of minimizing squared sum or errors is called least squared method of regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS technique tries to reduce the sum of squared errors ∑[Actual(y) - Predicted(y')]² by finding the best possible value of regression coefficients (b0, b1, etc)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "b1 = Σ(xi - xmean)(yi-ymean)/ Σ (xi - xmean)² where i= 1 to n (no. of obs.)\n",
    "bo = ymean - b1(xmean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In OLS, the error estimates can be divided into three parts:\n",
    "\n",
    "** Sum of Squares error (SSE) - ∑[Actual(y) - Predicted(y)]² **\n",
    "\n",
    "** Sum of Squares regression (SSR) - ∑[Predicted(y) - Mean(ymean)]² **\n",
    "\n",
    "** Sum of Squares Total (SST) - ∑[Actual(y) - Mean(ymean)]² **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](anat.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important use of these error terms is used in the calculation of the Coefficient of Determination (R²)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                                        `R² = 1 - (SSE/SST)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R² metric tells us the amount of variance explained by the independent variables in the model. \n",
    "\n",
    "### The assumptions made in regression \n",
    "\n",
    "The following are the major assumptions made by standard linear regression models with standard estimation techniques (e.g. ordinary least squares):\n",
    "\n",
    "* *** Linearity: *** This means that the response variable is a linear combination of the parameters (regression coefficients) and the predictor variables. It means that the change in DV by 1 unit change in IV is constant. \n",
    "\n",
    "* *** Lack of perfect multicollinearity: *** For standard least squares estimation methods there must be no correlation among independent variables. Presence of correlation in independent variables lead to Multicollinearity. If variables are correlated, it becomes extremely difficult for the model to determine the true effect of Xs on Y.\n",
    "\n",
    "* *** Constant variance (Homoscedasticity): *** This means that different values of the response variable have the same variance in their errors, regardless of the values of the predictor variables. Absence of constant variance leads to heteroskedestacity.\n",
    "\n",
    "* *** No auto-correlation: *** The error terms must be uncorrelated i.e. error at ∈t must not indicate the at error at ∈t+1. Presence of correlation in error terms is known as Autocorrelation. It drastically affects the regression coefficients and standard error values since they are based on the assumption of uncorrelated error terms.\n",
    "\n",
    "* The dependent variable and the residuals must possess a normal distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How check if  these assumptions are violated \n",
    " * ** Residual vs. Fitted Values Plot ** --  this plot shouldn't show any pattern. But if you see any shape (curve, U shape), it suggests non-linearity in the data set. In addition, if you see a funnel shape pattern, it suggests your data is suffering from heteroskedasticity, i.e. the error terms have non-constant variance.\n",
    " \n",
    " ![](het1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are a non-graphical person, you can also perform quick tests / methods to check assumption violations:\n",
    "\n",
    "* Durbin Watson Statistic (DW) - This test is used to check autocorrelation. Its value lies between 0 and 4. A DW=2 value shows no autocorrelation. However, a value between 0 < DW < 2 implies positive autocorrelation, while 2 < DW < 4 implies negative autocorrelation.\n",
    "* Variance Inflation Factor (VIF) - This metric is used to check multicollinearity. VIF <=4 implies no multicollinearity but VIF >=10 suggests high multicollinearity. Alternatively, you can also look at the tolerance (1/VIF) value to determine correlation in IVs. In addition, you can also create a correlation matrix to determine collinear variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to improve the accuracy of a regression model \n",
    "\n",
    "There is little you can do when your data violates regression assumptions. Following are some tips you can implement:\n",
    "\n",
    "* If your data is suffering from non-linearity, transform the IVs using sqrt, log, square, etc.\n",
    "* If your data is suffering from heteroskedasticity, transform the DV using sqrt, log, square, etc. Also, you can use weighted least square method to tackle this problem.\n",
    "* If your data is suffering from multicollinearity, use a correlation matrix to check correlated variables. Let's say variables A and B are highly correlated. Now, instead of removing one of them, use this approach: Find the average correlation of A and B with the rest of the variables. Whichever variable has the higher average in comparison with other variables, remove it. Alternatively, you can use penalized regression methods such as lasso, ridge, elastic net, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can you access the fit of regression model?\n",
    "\n",
    "The metrics used to determine model fit can have different values based on the type of data. Following are some metrics you can use to evaluate your regression model:\n",
    "\n",
    "* **R Square (Coefficient of Determination)** - As explained above, this metric explains the percentage of variance explained by covariates in the model. It ranges between 0 and 1. Usually, higher values are desirable but it rests on the data quality and domain. For example, if the data is noisy, you'd be happy to accept a model at low R² values. But it's a good practice to consider adjusted R² than R² to determine model fit.\n",
    "* **Adjusted R²**- The problem with R² is that it keeps on increasing as you increase the number of variables, regardless of the fact that the new variable is actually adding new information to the model. To overcome that, we use adjusted R² which doesn't increase (stays same or decrease) unless the newly added variable is truly useful.\n",
    "![](adjr2.gif)\n",
    "\n",
    "* **RMSE / MSE / MAE** - Error metric is the crucial evaluation number we must check. Since all these are errors, lower the number, better the model. Let's look at them one by one:\n",
    " * MSE - This is mean squared error. It tends to amplify the impact of outliers on the model's accuracy. For example, suppose the actual y is 10 and predictive y is 30, the resultant MSE would be (30-10)² = 400.\n",
    " * MAE - This is mean absolute error. It is robust against the effect of outliers. Using the previous example, the resultant MAE would be (30-10) = 20\n",
    " * RMSE - This is root mean square error. It is interpreted as how far on an average, the residuals are from zero. It nullifies squared effect of MSE by square root and provides the result in original units as data. Here, the resultant RMSE would be √(30-10)² = 20. Don't get baffled when you see the same value of MAE and RMSE. Usually, we calculate these numbers after summing overall values (actual - predicted) from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
