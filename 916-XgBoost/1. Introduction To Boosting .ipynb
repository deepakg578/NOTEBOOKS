{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "* What is boosting ?\n",
    "* Boosting algorithm\n",
    "  * Adaptive Boosting(Ada boost)\n",
    "  * Gradient Boosting(GBM)\n",
    "  * Extreme Gradient Boosting(XGBoost)\n",
    "* Algorithm main Parameters\n",
    "* Busilding Model\n",
    "* Finetuning Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Boosting ?\n",
    "\n",
    "* If bagging is wisdom of crowds then boosting is wisdom of crowds with each idividual given weight based on their expertise\n",
    "\n",
    "* Boosting in general decreases Bias error and builds strong predictive models\n",
    "\n",
    "* Boosting is an iterative technique. We adjust the weight of the observation based on previous classification\n",
    "\n",
    "* If an observation was classified incorrectly, it tries to increase the weight of this observation and vice versa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting algorithm\n",
    "\n",
    "![](b1.png)\n",
    "![](b2.png)\n",
    "![](b3.png)\n",
    "![](b4.png)\n",
    "![](b5.png)\n",
    "![](b6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Ada Boosting **\n",
    "\n",
    "Adaptive Boosting : Above method is of Ada Boost technique. Here we give high weight to misclassified records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Gradient Boosting **\n",
    "\n",
    "* Simillar to Ada boosting algorithm\n",
    "* The approach is same but there are slight modification during re-weighted sampling\n",
    "* We update the weights based on misclassification rate and gradient decent\n",
    "* It give better result in some class of problem like regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Gradient Descent:*\n",
    "Gradient descent tries to optimize the loss function by tuning different values of coefficients to minimize the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** XGBoost **\n",
    "\n",
    "* Follows the principle of gradient boosting\n",
    "* It uses a more regularized model formalization to control over-fitting, which gives better performance\n",
    "* It uses improved convergence techniques, vector and matrix type data structure for faster result\n",
    "* Better support for multicore processing which reduces overall training time\n",
    "* For a given dataset you are less likely to get memory error while using xgboost when compared to GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm main Parameters\n",
    "\n",
    "XGBoost parameters can be divided into three categories :\n",
    "\n",
    "* General Parameters: Controls the booster type in the model which eventually drives overall functioning\n",
    "* Booster Parameters: Controls the performance of the selected booster\n",
    "* Learning Task Parameters: Sets and evaluates the learning process of the booster from the given data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** General Parameters **\n",
    "* Booster[default=gbtree]\n",
    "    * Sets the booster type (gbtree, gblinear or dart) to use. For classification problems, you can use gbtree, dart. \n",
    "* nthread[default=maximum cores available] \n",
    "    * Activates parallel computation. Generally, people don't change it as using maximum cores leads to the fastest computation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Booster Parameters **\n",
    "\n",
    "* n_estimators[default=100]\n",
    "    * It controls the maximum number of iterations. For classification, it is similar to the number of trees to grow.\n",
    "    * Should be tuned using CV\n",
    "* gamma[default=0][range: (0,Inf)]\n",
    "    * It controls regularization (or prevents overfitting). \n",
    "    * The optimal value of gamma depends on the data set and other parameter values.\n",
    "    * Higher the value, higher the regularization. Regularization means penalizing large coefficients which don't improve the model's performance. default = 0 means no regularization.\n",
    "    * Tune trick: Start with 0 and check CV error rate. If you see train error >>> test error, bring gamma into action.\n",
    "    * Higher the gamma, lower the difference in train and test CV. If you have no clue what value to use, use gamma=5 and see the performance. \n",
    "    * Remember that gamma brings improvement when you want to use shallow (low max_depth) trees.\n",
    "* max_depth[default=3][range: (0,Inf)]\n",
    "    * It controls the depth of the tree.\n",
    "    * Larger the depth, more complex the model; higher chances of overfitting. There is no standard value for max_depth. \n",
    "    * Larger data sets require deep trees to learn the rules from data.\n",
    "\n",
    "* min_child_weight[default=1][range:(0,Inf)]\n",
    "    * In classification, if the leaf node has a minimum sum of instance weight (calculated by second order partial derivative) lower than min_child_weight, the tree splitting stops.\n",
    "    * In simple words, it blocks the potential feature interactions to prevent overfitting. Should be tuned using CV.\n",
    "* subsample[default=1][range: (0,1)]\n",
    "    * It controls the number of samples (observations) supplied to a tree.\n",
    "    * Typically, its values lie between (0.5-0.8)\n",
    "* colsample_bytree[default=1][range: (0,1)]\n",
    "    * It control the number of features (variables) supplied to a tree\n",
    "    * Typically, its values lie between (0.5,0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Learning Task Parameters **\n",
    "\n",
    "* Objective[default=reg:linear]\n",
    "    * reg:linear - for linear regression\n",
    "    * binary:logistic - logistic regression for binary classification. It returns class probabilities\n",
    "    * multi:softmax - multiclassification using softmax objective. It returns predicted class labels. It requires setting num_class parameter denoting number of unique prediction classes.\n",
    "    * multi:softprob - multiclassification using softmax objective. It returns predicted class probabilities.\n",
    "* eval_metric [no default, depends on objective selected]\n",
    "    * These metrics are used to evaluate a model's accuracy on validation data. For classification, default metric is error.\n",
    "    * Available error functions are as follows:\n",
    "        * mae - Mean Absolute Error (used in regression)\n",
    "        * Logloss - Negative loglikelihood (used in classification)\n",
    "        * AUC - Area under curve (used in classification)\n",
    "        * error - Binary classification error rate [#wrong cases/#all cases]\n",
    "        * mlogloss - multiclass logloss (used in classification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
