{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "* What is Clustering?\n",
    "* Distance Calculation for Clustering\n",
    "    * Euclidean Distance\n",
    "    * Manhattan Distance\n",
    "    * Hamming Distance\n",
    "    * Gower Distance\n",
    "    * Cosine Similarity\n",
    "* Types of Clustering Techniques\n",
    "    * K-Means\n",
    "        * Algorithm\n",
    "        * How to initialize K centroids?\n",
    "        * How to calculate centroid of a cluster?\n",
    "        * How to find value of K for the dataset?\n",
    "    * K Modes / KPrototypes\n",
    "    * Hierarchical (Agglomerative )\n",
    "        * Algorithm\n",
    "        * How to calculate distance between two clusters?\n",
    "        * How many clusters to form?\n",
    "    * Advantages of K-Means over Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Clustering?\n",
    "\n",
    "It’s an unsupervised learning algorithm which groups the given data, such that data points with similar behaviour are merged into one group.\n",
    "\n",
    "Main aim is to segregate the various data points into different groups called clusters such that entities in a particular group comparatively have more similar traits than entities in another group. \n",
    "\n",
    "Clustering algorithm does not predict an outcome or target variable but can be used to improve predictive model. Predictive models can be built for clusters to improve the accuracy of our prediction.\n",
    "![](cl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Calculation for Clustering\n",
    "\n",
    "There are some important things you should keep in mind:\n",
    "\n",
    "* With quantitative variables, distance calculations are highly influenced by variable units and magnitude. For example, clustering variable height (in feet) with salary (in rupees) having different units and distribution (skewed) will invariably return biased results. Hence, always make sure to standardize (mean = 0, sd = 1) the variables. Standardization results in unit-less variables.\n",
    "* Use of a particular distance measure depends on the variable types; i.e., formula for calculating distance between numerical variables is different than categorical variables.\n",
    "\n",
    "Suppose, we are given a 2-dimensional data with xi = (xi1, xi2, . . . , xip) and xj = (xj1, xj2, . . . , xjp). Both are numeric variables. We can calculate various distances as follows:\n",
    "\n",
    "* ** Euclidean Distance:** It is used to calculate the distance between quantitative (numeric) variables. As it involves square terms, it is also known as L2 distance (because it squares the difference in coordinates). Its formula is given by"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                    d(xi , xj ) = (|xi1 − xj1|² + |xi2 − xj2|² + . . . + |xip − xjp|² ) 1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ** Manhattan Distance:** It is calculated as the absolute value of the sum of differences in the given coordinates. This is known as L1 distance. It is also sometimes called the Minowski Distance."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                    d(xi , xj ) = (|xi1 − xj1| + |xi2 − xj2| + . . . + |xip − xjp|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Hamming Distance:** It is used to calculate the distance between categorical variables. It uses a contingency table to count the number of mismatches among the observations. If a categorical variable is binary (say, male or female), it encodes the variable as male = 0, female = 1.\n",
    "\n",
    "In case a categorical variable has more than two levels, the Hamming distance is calculated based on dummy encoding. Its formula is given by (x,y are given points):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                      hdist(x, y) <- sum((x[1] != y[1]) + (x[2] != y[2]) + ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Gower Distance:** It is used to calculate the distance between mixed (numeric, categorical) variables. It works this way: it computes the distance between observations weighted by its variable type, and then takes the mean across all variables.\n",
    "\n",
    "Technically, the above-mentioned distance measures are a form of Gower distances; i.e. if all the variables are numeric in nature, Gower distance takes the form of Euclidean. If all the values are categorical, it takes the form of Manhattan or Jaccard distance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Cosine Similarity:** It is the most commonly used similarity metric in text analysis. The closeness of text data is measured by the smallest angle between two vectors. The angle (Θ) is assumed to be between 0 and 90. A quick refresher: cos (Θ = 0) = 1 and cos (Θ = 90) = 0.\n",
    "\n",
    "Therefore, the maximum dissimilarity between two vectors is measured at Cos 90 (perpendicular). And, two vectors are said to be most similar at Cos 0 (parallel). For two vectors (x,y), the cosine similarity is given by their normalized dot product shown below:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                     cossim(x, y) <- dot(x, y)/(sqrt(dot(x,x)*dot(y,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Clustering Techniques\n",
    "\n",
    "There exist more than 100 clustering algorithms as of today.\n",
    "Some of the commonly used are k-Means, Hierarchical, DBSCAN and OPTICS\n",
    "We will be discussing about k-Means and Hierarchical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means\n",
    " It involves a simple way to classify the data set into fixed no. of K clusters . The idea is to define K centroids, one for each cluster.\n",
    "The final clusters depend on the initial configuration of centroids. So, they should be initialized as far from each other as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Algorithm **\n",
    "* Let there be N data points. At first, K centroids are initialised in our data set representing K different clusters.\n",
    "* Now, each of the N data points are assigned to closest centroid in the data set and merged with that centroid as a single cluster. In this way, every data point is assigned to one of the centroids.\n",
    "* Then, K cluster centroids are recalculated and again, each of the N data points are assigned to the nearest centroid.\n",
    "* Step 3 is repeated until no further improvement can be made.\n",
    "\n",
    "This algorithm aims at minimising the objective function:\n",
    "![](eclu.jpeg)\n",
    "\n",
    "It represent the sum of **euclidean distance** of all the data points from the cluster centroid which is minimised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** How to initialize K centroids? **\n",
    "* Forgy: Randomly assigning K centroid points in our data set.\n",
    "* Random Partition: Assigning each data point to a cluster randomly, and then proceeding to evaluation of centroid positions of each cluster.\n",
    "* KMeans++: Used for small data sets.\n",
    "* Canopy Clustering: Unsupervised pre-clustering algorithm used as preprocessing step for K-Means or any Hierarchical Clustering. It helps in speeding up clustering operations on large data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** How to calculate centroid of a cluster? **\n",
    "\n",
    "Simply the mean of all the data points within that cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** How to find value of K for the dataset? **\n",
    "\n",
    "* Elbow Method: Clustering is done on a dataset for varying values of K and SSE (Sum of squared errors) is calculated for each value of K. \n",
    "Then, a graph between K and SSE is plotted. Plot formed assumes the shape of an arm. There is a point on the graph where SSE does not decreases significantly with increasing K. This is represented by elbow of the arm and is chosen as the value of K. (OPTIMUM)\n",
    "![](elbow.png)\n",
    "\n",
    "#### DEFINITION of 'Sum Of Squares'\n",
    "Sum of Squares is a statistical technique used in regression analysis to determine the dispersion of data points. In a regression analysis, the goal is to determine how well a data series can be fitted to a function which might help to explain how the data series was generated. The sum of squares is used as a mathematical way to find the function which best fits (varies least) from the data.\n",
    "\n",
    "The calculation for Sum of Squares is ∑(xi – x̄)2\n",
    "\n",
    "Sum of squares is also known as Variation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KModes / Kprototype\n",
    "\n",
    "* The basic concept of k-means stands on mathematical calculations (means, euclidian distances). \n",
    "* But what if our data is non-numerical or, in other words, categorical?\n",
    "* We could think of transforming our categorical values in numerical values and eventually apply k-means. But beware: k-means uses numerical distances, so it could consider close two really distant objects that merely have been assigned two close numbers.\n",
    "\n",
    "** Definition **\n",
    "\n",
    "* k-modes is an extension of k-means. Instead of distances it uses dissimilarities (that is, quantification of the total mismatches between two objects: the smaller this number, the more similar the two objects)\n",
    "* And instead of means, it uses modes. A mode is a vector of elements that minimizes the dissimilarities between the vector itself and each object of the data. We will have as many modes as the number of clusters we required, since they act as centroids.\n",
    "* For numerical and categorical data, another extension of these algorithms exists, basically combining k-means and k-modes. It is called k-prototypes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical / Agglomerative \n",
    "\n",
    "** Algorithm **\n",
    "\n",
    "* Let there be N data points. Firstly, these N data points are assigned to N different clusters with one data point in each cluster.\n",
    "* Then, two data points with minimum euclidean distance between them are merged into a single cluster.\n",
    "* Then, two clusters with minimum centroid distance between them are merged into a single cluster.\n",
    "* This process is repeated until we are left with a single cluster, hence forming hierarchy of clusters.\n",
    "\n",
    "![](aggl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** How to calculate distance between two clusters? **\n",
    "\n",
    "* Centroid Distance: Euclidean distance between mean of data points in the two clusters\n",
    "* Minimum Distance: Euclidean distance between two data points in the two clusters that are closest to each other\n",
    "* Maximum Distance : Euclidean distance between two data points in the two clusters that are farthest to each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** How many clusters to form? **\n",
    "\n",
    "* Visualising dendogram: Best choice of no. of clusters is no. of vertical lines that can be cut by a horizontal line, that can transverse maximum distance vertically without intersecting other cluster. \n",
    "For eg., in the below case, best choice for no. of clusters will be 4.\n",
    "The y-axis is a measure of closeness of either individual data points or clusters.\n",
    "![](dendo.png)\n",
    "\n",
    "* Intuition and prior knowledge of the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of K-Means over Hierarchical clustering\n",
    "\n",
    "* It uses less memory.\n",
    "* It converges faster.\n",
    "* Unlike hierarchical, k means doesn't get trapped in mistakes made on a previous level. It improves iteratively.\n",
    "* k means is non-deterministic in nature, i.e.. after every time you initialize, it will produce different clusters. On the contrary, hierarchical clustering is deterministic.\n",
    "\n",
    "**Note:** K means is preferred when the data is numeric. Hierarchical clustering is preferred when the data is categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
